<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 模型 | 因果模型学习笔记</title>
  <meta name="description" content="4 模型 | 因果模型学习笔记" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="4 模型 | 因果模型学习笔记" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 模型 | 因果模型学习笔记" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2025-09-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="基础定义.html"/>
<link rel="next" href="treatment类型.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="le_uplift.html"><a href="le_uplift.html"><i class="fa fa-check"></i><b>1</b> le_uplift</a></li>
<li class="chapter" data-level="2" data-path=""><a href="#uplift-model%E4%B8%8Eresponse-model%E5%8C%BA%E5%88%AB"><i class="fa fa-check"></i><b>2</b> uplift-model与response-model区别</a></li>
<li class="chapter" data-level="3" data-path=""><a href="#%E5%9F%BA%E7%A1%80%E5%AE%9A%E4%B9%89"><i class="fa fa-check"></i><b>3</b> 基础定义</a></li>
<li class="chapter" data-level="4" data-path=""><a href="#%E6%A8%A1%E5%9E%8B"><i class="fa fa-check"></i><b>4</b> 模型</a>
<ul>
<li class="chapter" data-level="4.1" data-path="模型.html"><a href="模型.html"><i class="fa fa-check"></i><b>4.1</b> meta-learner</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="模型.html"><a href="模型.html#slearner"><i class="fa fa-check"></i><b>4.1.1</b> Slearner</a></li>
<li class="chapter" data-level="4.1.2" data-path="模型.html"><a href="模型.html#tlearner"><i class="fa fa-check"></i><b>4.1.2</b> TLearner</a></li>
<li class="chapter" data-level="4.1.3" data-path="模型.html"><a href="模型.html#xlearner"><i class="fa fa-check"></i><b>4.1.3</b> XLearner</a></li>
<li class="chapter" data-level="4.1.4" data-path="模型.html"><a href="模型.html#rlearner"><i class="fa fa-check"></i><b>4.1.4</b> RLearner</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="模型.html"><a href="模型.html#tree-base"><i class="fa fa-check"></i><b>4.2</b> Tree-base</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="模型.html"><a href="模型.html#casusaltree"><i class="fa fa-check"></i><b>4.2.1</b> Casusaltree</a></li>
<li class="chapter" data-level="4.2.2" data-path="模型.html"><a href="模型.html#casusalforest"><i class="fa fa-check"></i><b>4.2.2</b> CasusalForest</a></li>
<li class="chapter" data-level="4.2.3" data-path="模型.html"><a href="模型.html#dml"><i class="fa fa-check"></i><b>4.2.3</b> DML</a></li>
<li class="chapter" data-level="4.2.4" data-path="模型.html"><a href="模型.html#drl"><i class="fa fa-check"></i><b>4.2.4</b> DRL</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path=""><a href="#%E6%B7%B1%E5%BA%A6%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B"><i class="fa fa-check"></i><b>4.3</b> 深度因果模型</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="模型.html"><a href="模型.html#tarnet"><i class="fa fa-check"></i><b>4.3.1</b> TARNET</a></li>
<li class="chapter" data-level="4.3.2" data-path="模型.html"><a href="模型.html#dragonnet"><i class="fa fa-check"></i><b>4.3.2</b> Dragonnet</a></li>
<li class="chapter" data-level="4.3.3" data-path="模型.html"><a href="模型.html#efin"><i class="fa fa-check"></i><b>4.3.3</b> EFIN</a></li>
<li class="chapter" data-level="4.3.4" data-path="模型.html"><a href="模型.html#descn"><i class="fa fa-check"></i><b>4.3.4</b> DESCN</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path=""><a href="#treatment%E7%B1%BB%E5%9E%8B"><i class="fa fa-check"></i><b>5</b> treatment类型</a>
<ul>
<li class="chapter" data-level="5.1" data-path=""><a href="#%E4%BA%8C%E5%85%83-treatment"><i class="fa fa-check"></i><b>5.1</b> 二元 treatment</a></li>
<li class="chapter" data-level="5.2" data-path=""><a href="#%E5%A4%9A%E5%85%83treatment"><i class="fa fa-check"></i><b>5.2</b> 多元treatment</a></li>
<li class="chapter" data-level="5.3" data-path=""><a href="#%E8%BF%9E%E7%BB%ADtreatment"><i class="fa fa-check"></i><b>5.3</b> 连续treatment</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path=""><a href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><i class="fa fa-check"></i><b>6</b> 数据收集</a>
<ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#rct%E6%95%B0%E6%8D%AE"><i class="fa fa-check"></i><b>6.1</b> RCT数据</a></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#%E8%A7%82%E6%B5%8B%E6%95%B0%E6%8D%AE"><i class="fa fa-check"></i><b>6.2</b> 观测数据</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path=""><a href="#%E7%BA%A0%E5%81%8F"><i class="fa fa-check"></i><b>7</b> 纠偏</a>
<ul>
<li class="chapter" data-level="7.1" data-path=""><a href="#%E5%B8%B8%E8%A7%81%E5%81%8F%E5%B7%AE"><i class="fa fa-check"></i><b>7.1</b> 常见偏差</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path=""><a href="#%E8%AF%84%E4%BC%B0"><i class="fa fa-check"></i><b>8</b> 评估</a>
<ul>
<li class="chapter" data-level="8.1" data-path="评估.html"><a href="评估.html"><i class="fa fa-check"></i><b>8.1</b> AUUC</a></li>
<li class="chapter" data-level="8.2" data-path="评估.html"><a href="评估.html#qini_score"><i class="fa fa-check"></i><b>8.2</b> QINI_SCORE</a></li>
<li class="chapter" data-level="8.3" data-path="评估.html"><a href="评估.html#section"><i class="fa fa-check"></i><b>8.3</b> </a></li>
<li class="chapter" data-level="8.4" data-path=""><a href="#%E5%BA%8F%E5%87%86"><i class="fa fa-check"></i><b>8.4</b> 序准</a></li>
<li class="chapter" data-level="8.5" data-path=""><a href="#%E5%80%BC%E5%87%86"><i class="fa fa-check"></i><b>8.5</b> 值准</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path=""><a href="#%E9%A2%84%E7%AE%97%E5%88%86%E9%85%8D"><i class="fa fa-check"></i><b>9</b> 预算分配</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">因果模型学习笔记</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="模型" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> 模型<a href="#%E6%A8%A1%E5%9E%8B" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="meta-learner" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> meta-learner<a href="模型.html#meta-learner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="slearner" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Slearner<a href="模型.html#slearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>s 顾名思义：single model，就是一个模型都搞定
这个模型最简单，直接把treatment作为特征放进模型来预测。首先我们把 T 作为特征一起放进机器学习模型的特征， Y 是目标，然后训练一个有监督的模型 <span class="math inline">\(\mu(x)=E[Y \mid X=x, T=t]\)</span> 。然后我们改变 T 的值，就可以得到两个不同的结果，再一相减就好了：
<span class="math display">\[\hat{\tau}(x)=\hat{\mu}(x, T=1)-\hat{\mu}(x, T=0)\]</span>
<img src="refs/slearner.jpg" />
优点：简单</p>
<p>缺点：
- 1. 本质上还是不是对 uplift 直接进行建模，因此从效果上来说还是有提升空间。
- 2. S-Learner 倾向于将干预效果趋近于0，尤其是特征的维度非常大的时候，模型在训练的过程中极容易忽略这个干预变量。同时模型中正则化的引入，也会带来干预变量的效果稀释，正则化越强，该问题就会越大。</p>
<p>代码</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="tlearner" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> TLearner<a href="模型.html#tlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>T 顾名思义：two model 需要用俩模型（可以是任何的response模型）</p>
<p>对于 treatment 组的样本和 control 组的样本分别独立训练一个响应模型 <span class="math inline">\(\hat{\mu}_1(x)\)</span>和 <span class="math inline">\(\hat{\mu}_0(x)\)</span> 。其中 <span class="math inline">\(\hat{\mu}_1(x)\)</span> 用于拟合在施加干预的情况下响应目标 <span class="math inline">\(Y\)</span> 与特征 <span class="math inline">\(X\)</span> 的关系，即 <span class="math inline">\(\mu_1(x)=E[Y \mid W=1, X=x]\)</span> ；而 <span class="math inline">\(\hat{\mu}_0(x)\)</span> 用于拟合在未施加干预的情况下响应目标 <span class="math inline">\(Y\)</span>与特征 <span class="math inline">\(X\)</span> 的关系，即 <span class="math inline">\(\mu_0(x)=E[Y \mid W=0, X=x]\)</span> 。</p>
<p>之后拿训练好的两个模型对于同一个样本 <span class="math inline">\(x\)</span> 的预测结果做差，就得到了 uplift 结果。</p>
<p>最终得到的 T－Learner 表达式为 <span class="math inline">\(\hat{\tau}_T(x)=\hat{\mu}_1(x)-\hat{\mu}_0(x)\)</span>
下面是 T－Learner 算法的示意图（参考 Causal Inference for The Brave and True 这本书，因此该图部分 notation 与上文存在差异：图中的 T 对应上文的干预 <span class="math inline">\(W\)</span> ；图中的 M1和 M 0 对应上文的响应模型 <span class="math inline">\(\hat{\mu}_1\)</span> 和 <span class="math inline">\(\hat{\mu}_0\)</span> ）<a href="https://zhuanlan.zhihu.com/p/682079211">参考</a></p>
<div class="float">
<img src="refs/tlearner.jpg" alt="tlearner" />
<div class="figcaption">tlearner</div>
</div>
<p>优点：可以灵活地使用已有的机器学习方法。对干预和非干预样本分别建模，充分考虑了干预因素的影响。</p>
<p>缺点：
- 1. T-Learner 并不是直接对 uplift 进行建模，因此对 uplift 的识别能力有限。
- 2. 考虑到其基于两个独立训练的模型进行二次处理后来对 uplift 进行预测，很容易产生两个独立模型的误差累积的问题。</p>
</div>
<div id="xlearner" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> XLearner<a href="模型.html#xlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>1．参考T-learner的思路，先对于 <span class="math inline">\(T=0\)</span> 的control组和 <span class="math inline">\(T=1\)</span> 的treatment组分别学习一个有监督的模型。
<img src="refs/xlearner2.jpg" alt="xlearner" /></p>
<p><span class="math display">\[
\begin{aligned}
&amp; \mu_0(x)=E\left[Y^0 \mid X=x\right] \\
&amp; \mu_1(x)=E\left[Y^1 \mid X=x\right]
\end{aligned}
\]</span></p>
<p>2．然后对于 <span class="math inline">\(\mathrm{T}=1\)</span> 的样本和 <span class="math inline">\(\mathrm{T}=0\)</span> 的样本，分别使用 <span class="math inline">\(\mu_0(x)\)</span> 和 <span class="math inline">\(\mu_1(x)\)</span> 预测一个 <span class="math inline">\(\hat{\mu}^0\left(X^1\right)\)</span> 和 <span class="math inline">\(\hat{\mu}^1\left(X^0\right)\)</span> ，这里 <span class="math inline">\(X^0, X^1\)</span> 分别是 <span class="math inline">\(\mathrm{T}=0, \mathrm{~T}=1\)</span> 组 的样本，这步就是就是获得一个反事实的结果 （比如对于某一个 <span class="math inline">\(\mathrm{T}=1\)</span> 的样本 <span class="math inline">\(X_i^1\)</span> ，事实结果是 <span class="math inline">\(Y_i^1\)</span> ，反事实结果是 <span class="math inline">\(\hat{\mu}^0\left(X_i^1\right)\)</span> 。于是我们就可以对于control组和treatment组分别计算difference <span class="math inline">\(D_i^0\)</span> 和 <span class="math inline">\(D_i^1\)</span> ：</p>
<p><span class="math display">\[
\begin{aligned}
D_i^1 &amp; =Y_i^1-\hat{\mu}^0\left(X_i^1\right) \\
D_i^0 &amp; =\hat{\mu}^1\left(X_i^0\right)-Y_i^0
\end{aligned}
\]</span></p>
<p>3．最终对于一个新样本的CATE就是这两个的加权平均，权重是什么呢？一般是propensity score <span class="math inline">\(g(x)=P(T=1 \mid X=x)\)</span> ，这个可以通过一个LR模型或者任何二分类模型得到得到。最后新样本的的CATE如下：<span class="math inline">\(\hat{\tau}(x)=g(x) \hat{\tau}_0(x)+(1-g(x)) \hat{\tau}_1(x)\)</span></p>
<p><img src="refs/xlearner.jpg" alt="xlearner" />
下面是 X－Learner 算法的示意图（同样参考 Causal Inference for The Brave and True 这本书，图中 notation 与上文存在差异：图中的 T 对应上文的干预 <span class="math inline">\(W\)</span> ；图中的 M0 和 M1对应前面的模型 <span class="math inline">\(\hat{\mu}_0\)</span> 和 <span class="math inline">\(\hat{\mu}_1\)</span> ；图中的 MTAU0 和 MTAU1 对应前面的模型 <span class="math inline">\(\hat{\tau}_0\)</span> 和 <span class="math inline">\(\hat{\tau}_1\)</span> ）<a href="https://zhuanlan.zhihu.com/p/682079211">参考</a></p>
</div>
<div id="rlearner" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> RLearner<a href="模型.html#rlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="tree-base" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Tree-base<a href="模型.html#tree-base" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>传统机器学习模型中，树模型主要的思路就是通过对特征点进行分裂，将X划分到一个又一个subspace中，这与补贴场景下，希望找到某一小部分增量很高的用户的想法几乎是完美重合。</p>
<p>传统分类树模型是希望通过信息理论(information theory)中的信息熵等思想，用计算信息增益的方法去解决分类问题。而在uplift tree model中，其本质也还是想要通过衡量分裂前后的变量差值去决策是否分裂节点，不过这里的这个决策差值的计算方法不再是信息增益(information gain)，而是不同的直接对增量uplift建模的计算方法，其中包括了利用分布散度对uplift建模和直接对uplift建模。</p>
<p>下面介绍三个Tree-Based算法，Uplift-Tree，CausalForest，CTS。</p>
<div id="casusaltree" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Casusaltree<a href="模型.html#casusaltree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>causal tree <span class="math inline">\({ }^{+}\)</span>（honest tree）是一种直接对目标进行建模的方法，它改进了传统决策树的优化目标和指标分桶方式，以达到最大化分桶的异质性因果效应，同时调整误差的效果。</p>
<p>首先，它会把数据分成训练集和估计集，一部分训练集去构造树，另外一部分估计集去估计因果效应和方差。</p>
<p>传统决策树将SSE作为目标函数，而causal tree的目标函数如下：</p>
<p><span class="math display">\[
F\left(S_l\right)=N_l * \tau^2\left(S_l\right)-N_l\left(\frac{\operatorname{Var}\left(S_l, 1\right)}{p}+\frac{\operatorname{Var}\left(S_l, 0\right)}{1-p}\right)
\]</span></p>
<p>其中，前半部分代表实验组的treatment effect，后半部分代表实验组和对照组的 variance。</p>
<p>通俗理解构建causal tree的过程：首先将训练机划分为训练集和评估集两部分，用训练集训练生成一颗决策树，训练的目标函数同时考虑了实验效应（最大）和方差（最小）。然后用评估集来估计CATE作为该叶子结点的CATE，对于新样本将会用该CATE作为预测值。<a href="https://zhuanlan.zhihu.com/p/688168539">参考</a></p>
</div>
<div id="casusalforest" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> CasusalForest<a href="模型.html#casusalforest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>因果森林（Causal Forest）由 Susan Athey 和 Stefan Wager 等大牛提出（2019），本质是广义随机森林（GRF）在因果推断场景的特化应用。因果森林的核心思想在于将随机森林改造为一个专门用于估计异质性因果效应的强大工具。传统的回归树分裂准则是让数据集平均MSE最小化，因果森林是通过 uplift 增益导向的分裂准则，引导树结构主动寻找处理效应发生变化的边界，从而自适应地将样本划分为协变量和处理组／对照组都相对平衡的局部子群体（近似局部 RCT）。分裂标准表达式为：</p>
<p><span class="math display">\[
\Delta\left(C_1, C_2\right)=\frac{n_{C_1} n_{C_2}}{n_P^2}\left(\hat{\theta_{C_1}}-\hat{\theta_{C_2}}\right)^2
\]</span></p>
<p><span class="math inline">\(P\)</span>代表决策树父节点， <span class="math inline">\(C_1\)</span>和<span class="math inline">\(C_2\)</span>是分裂后的 2 个子节点，<span class="math inline">\(\theta\)</span> 是两个子集的平均处理效应。因为两个子集中既有 <span class="math inline">\(\mathrm{T}=1\)</span> 的样本，也有 <span class="math inline">\(\mathrm{T}=0\)</span> 的样本，所以可以计算出平均处理效应，即平均增益。<span class="math inline">\(n\)</span>代表各集合的样本数目。决策树分裂要求上述表达式的值最大化。</p>
<ul>
<li><span class="math inline">\(\Delta\left(C_1, C_2\right)\)</span> ：表示类别 <span class="math inline">\(C_1\)</span> 和 <span class="math inline">\(C_2\)</span> 之间的＂差异值＂，是整个公式要计算的核心结果。</li>
<li><span class="math inline">\(n_{C_1}\)</span> ：类别 <span class="math inline">\(C_1\)</span> 中的样本数量（或观测数）。</li>
<li><span class="math inline">\(n_{C_2}\)</span> ：类别 <span class="math inline">\(C_2\)</span> 中的样本数量（或观测数）。</li>
<li><span class="math inline">\(n_P\)</span> ：总体（或父群体、合并群体）的总样本数量（即 <span class="math inline">\(n_P=n_{C_1}+n_{C_2}\)</span> ，如果 <span class="math inline">\(C_1\)</span> 和 <span class="math inline">\(C_2\)</span> 是总体的仅有的两个子组）。</li>
<li><span class="math inline">\(\hat{\theta}_{C_1}\)</span> ：对类别 <span class="math inline">\(C_1\)</span> 中某个参数 <span class="math inline">\(\theta\)</span> 的估计值（比如均值、处理效应、概率等，具体含义由研究场景决定）。</li>
<li><span class="math inline">\(\hat{\theta}_{C_2}\)</span> ：对类别 <span class="math inline">\(C_2\)</span> 中参数 <span class="math inline">\(\theta\)</span> 的估计值。</li>
</ul>
<p>公式逻辑与意义
公式的核心是<strong><em>组间参数差异的加权平方</em></strong>，权重由＂组样本量占总体的比例＂决定：</p>
<ul>
<li>分子部分：<span class="math inline">\(n_{C_1} n_{C_2}\left(\hat{\theta}_{C_1}-\hat{\theta}_{C_2}\right)^2\)</span></li>
<li><span class="math inline">\(\left(\hat{\theta}_{C_1}-\hat{\theta}_{C_2}\right)^2\)</span> ：直接衡量两个组的参数估计值的＂平方差＂，差越大，组间差异越显著。</li>
<li><span class="math inline">\(n_{C_1} n_{C_2}\)</span> ：是对＂组样本量＂的加权——如果某组样本量很大，会让这部分权重更高，体现＂大样本组的差异更值得关注＂ （或更能代表群体特征）。</li>
<li>分母部分：<span class="math inline">\(n_P^2\)</span>
对分子的 “样本量加权” 做归一化，避免因总体样本量<span class="math inline">\(n_P\)</span>过大导致 <span class="math inline">\(Δ\)</span>数值无限制膨胀，让结果更具可比性。</li>
</ul>
<p>传统的回归树，叶子结点预测结果是叶子结点上样本的响应变量平均值。类似的，因果树预测的结果是，叶子结点上的平均 CATE，以发优惠券为例，叶子结点预测的结果就是该节点上发券样本的平均转化率－不发券样本的平均转化率。</p>
<p>因果森林对比随机森林的差异</p>
<table>
<colgroup>
<col width="13%" />
<col width="43%" />
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">差异点</th>
<th align="left">随机森林</th>
<th align="left">因果森林</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">目标</td>
<td align="left">目标是预测一个观测值的结果 Y（分类或回归）。<br>分裂准则（如基尼系数、信息增益、均方误差）旨在最大化节点内结果的同质性或最小化预测误差。</td>
<td align="left">核心目标直接是估计每个个体或相似群体（叶子节点）的处理效应 <span class="math inline">\(τ(x) = E[Y(1)-Y(0) \mid X=x]\)</span>。<br>它不再直接预测 Y，而是预测 <span class="math inline">\(Y(1) - Y(0)\)</span> 这个反事实差异。<br>森林中的每一棵树都旨在构建一个能将样本划分为处理效应尽可能同质的子群体（叶子节点）的结构。</td>
</tr>
<tr class="even">
<td align="left">分裂准则</td>
<td align="left">传统分裂准则（如 MSE）在存在处理变量T时，会倾向于选择那些能同时很好预测Y且与T 相关的变量（即混杂因素）。但这会导致分裂后的子节点内，处理组和对照组在协变量分布上仍然不平衡（混杂未完全消除），从而污染了处理效应的估计。</td>
<td align="left">因果森林的分裂准则专门设计用于最大化子节点间处理效应的异质性或最小化子节点内处理效应估计的方差，常用准则包括：<br>• 基于 CATE 差异最大化：在候选分裂点，计算分裂后左右子节点的 CATE（<span class="math inline">\(τ_L\)</span> 和 <span class="math inline">\(τ_R\)</span>）差异 <span class="math inline">\(|τ_L - τ_R|\)</span>，选择差异最大的变量和分裂点（驱动树寻找效应突变边界）；<br>• 基于负梯度（梯度提升视角）：将 CATE 估计视为优化问题，用处理效应损失函数（如均方处理效应误差）的负梯度作为伪残差，指导分裂方向以减少 CATE 估计误差。</td>
</tr>
<tr class="odd">
<td align="left">集成学习与Honest Estimation</td>
<td align="left">基于“bootstrap抽样+随机特征选择”构建多棵决策树，集成时直接对每棵树的预测结果（分类概率/回归值）取平均，无“分裂与效应估计的样本分离”设计，未针对因果效应优化无偏性。</td>
<td align="left">1. 集成逻辑：与随机森林类似，构建数百棵“因果树”，每棵树基于随机子样本+随机特征子集训练，引入随机性以降低方差；<br>2. 双重样本利用（Honest Estimation）：<br> • 训练样本（S1）：仅用于构建树结构（决定分裂规则），不参与效应计算；<br> • 估计样本（S2）：与S1无交集（或通过Out-of-bag抽样实现），仅“落入”已训练好的树的叶子节点，用于计算该节点的效应 <span class="math inline">\(τ_{leaf}\)</span>（处理组与对照组均值差）；<br>3. 核心优势：<br> • 防过拟合：避免用同一批样本（S1）既建结构又算效应，减少噪声导致的偏差；<br> • 无偏性：满足独立性条件时，可提供渐近无偏的CATE估计；<br>4. 最终预测：新样本输入所有因果树，取各树输出的 <span class="math inline">\(τ_{leaf}\)</span> 均值（或中位数）作为最终CATE结果，集成平均提升估计稳定性。</td>
</tr>
</tbody>
</table>
</div>
<div id="dml" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> DML<a href="模型.html#dml" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>DML（double machine learning）是指一种用于因果推断的方法，它结合了现代机器学习技术与传统计量经济学方法，旨在从观测数据中准确估计因果效应。具体来说，double machine learning（DML）通过两个独立的预测步骤来实现这一目标：</p>
<p>第一阶段：使用机器学习模型预测结果变量 Y 和处理变量 T 关于协变量 X 的值。这一过程涉及两个模型的训练：一个是预测Y，另一个是预测T。</p>
<p><span class="math display">\[
\begin{aligned}
&amp; Y^{\prime}=f_1(X)+\varepsilon \\
&amp; T^{\prime}=f_2(x)+\varepsilon
\end{aligned}
\]</span></p>
<p>第二阶段：计算第一阶段预测值与实际值的残差，即得到 Y 的残差和 T 的残差。这些残差去除了协变量X的影响。</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \Delta Y=Y-Y^{\prime} \\
&amp; \Delta T=T-T^{\prime}
\end{aligned}
\]</span></p>
<p>回归分析：最后，对这些残差进行线性回归，以估计处理变量 T 对结果变量 Y 的因果效应。</p>
<p><span class="math display">\[
\Delta Y=\mu(x) * \Delta T+\varepsilon
\]</span></p>
<p>最终 <span class="math inline">\(\mu(x)\)</span> 即为输出结果。
这种方法的核心在于构造＂正交化＂得分，即通过残差来消除协变量的混杂影响，从而实现无偏的因果效应估计。DML特别适用于高维数据和复杂非线性关系的情况，因为它能够利用灵活的机器学习模型来处理大量控制变量，同时保持估计的稳健性。将 DML里面的模型换成随机森林就是CausalForestDML。</p>
</div>
<div id="drl" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> DRL<a href="模型.html#drl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="深度因果模型" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> 深度因果模型<a href="#%E6%B7%B1%E5%BA%A6%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="tarnet" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> TARNET<a href="模型.html#tarnet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>深度因果模型的基础网络，很多后续的网络都是在此基础上拓展的.为了让神经网络学习到的表示相近，Shalit et al．（2017）提出了Treatment Agnostic Regression Network（TARNet）。它的思想很简单，神经网络整体为双头结构，然后像 multi－task一样的共享层去学习共有的信息表征 <span class="math inline">\(\phi(x)\)</span> ，再利用该共享表征去分别学习treated组和control的outcome结果。共享层其实就会在treated和control组的信息中进行tread off，从而达到一种balanced。
简言之：TARNet前半部分用的是整个数据集，但是在后两个分支分别用的是T=0和T=1的分组数据各自训练所属的分支。</p>
<p><img src="refs/tarnet.png" alt="tarnet" />
图a是T-Learner，图b是tarnet，其中tarnet在2元treatment的loss如下：<a href="https://zhuanlan.zhihu.com/p/603499723">参考</a>
<span class="math display">\[
\underset{h, \Phi}{\arg \min } \frac{1}{N} \sum_{i=1}^N \operatorname{MSE}(Y_i\left(T_i\right), \underbrace{h\left(\Phi\left(X_i\right), T_i\right)}_{\hat{Y}_i\left(T_i\right)})+\lambda \underbrace{\mathcal{R}(h)}_{L_2}
\]</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="模型.html#cb2-1" tabindex="-1"></a>def <span class="fu">regression_loss</span>(concat_true, concat_pred)<span class="sc">:</span></span>
<span id="cb2-2"><a href="模型.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="模型.html#cb2-3" tabindex="-1"></a>    y_true <span class="ot">=</span> concat_true[<span class="sc">:</span>, <span class="dv">0</span>]</span>
<span id="cb2-4"><a href="模型.html#cb2-4" tabindex="-1"></a>    t_true <span class="ot">=</span> concat_true[<span class="sc">:</span>, <span class="dv">1</span>]</span>
<span id="cb2-5"><a href="模型.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="模型.html#cb2-6" tabindex="-1"></a>    y0_pred <span class="ot">=</span> concat_pred[<span class="sc">:</span>, <span class="dv">0</span>]</span>
<span id="cb2-7"><a href="模型.html#cb2-7" tabindex="-1"></a>    y1_pred <span class="ot">=</span> concat_pred[<span class="sc">:</span>, <span class="dv">1</span>]</span>
<span id="cb2-8"><a href="模型.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="模型.html#cb2-9" tabindex="-1"></a>    loss0 <span class="ot">=</span> <span class="fu">tf.reduce_sum</span>((<span class="fl">1.</span> <span class="sc">-</span> t_true) <span class="sc">*</span> <span class="fu">tf.square</span>(y_true <span class="sc">-</span> y0_pred))</span>
<span id="cb2-10"><a href="模型.html#cb2-10" tabindex="-1"></a>    loss1 <span class="ot">=</span> <span class="fu">tf.reduce_sum</span>(t_true <span class="sc">*</span> <span class="fu">tf.square</span>(y_true <span class="sc">-</span> y1_pred))</span>
<span id="cb2-11"><a href="模型.html#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="模型.html#cb2-12" tabindex="-1"></a>    return loss0 <span class="sc">+</span> loss1</span></code></pre></div>
</div>
<div id="dragonnet" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Dragonnet<a href="模型.html#dragonnet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="efin" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> EFIN<a href="模型.html#efin" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="descn" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> DESCN<a href="模型.html#descn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>模型整体有两部分结构组成ESN网络+Xnetwork网络</p>
<div class="float">
<img src="refs/descn.png" alt="descn" />
<div class="figcaption">descn</div>
</div>
<p>解决的问题</p>
<ul>
<li><p>干预偏差问题 Treatment Bias： 即被干预Treatment组（实验组）和未干预Control 组（对照组）的分布是不同的。</p></li>
<li><p>样本不均衡问题 Sample Imbalance：干预的Treatment 组和未干预的Control 组样本差异大，样本不平衡。</p></li>
</ul>
<p>问题根源：Treatment Bias 和 Sample Imbalance 都源于非随机的观测数据。</p>
<p>Treatment Bias 是因为处理分配（W）与协变量也就是特这（X）和潜在结果（Y(0), Y(1)）不独立，即存在混淆变量。</p>
<p>Sample Imbalance 虽然有时是策略本身导致的（如只对1%的用户发券），但在非随机环境下，这种数量上的不平衡会加剧模型学习的难度。</p>
<p>理论上来说，RCT实验数据是可以一定程度避免这俩问题的。
本文方法的三个假设：</p>
<ul>
<li><p>一致性Consistency：<span class="math inline">\(y_i=y_i\left(w_i\right)\)</span> ，观测到的 <span class="math inline">\(y_i\)</span> 和 潜在的 <span class="math inline">\(y_i\left(w_i\right)\)</span> 是一致的，即干预对结果的影响是稳定的。</p></li>
<li><p>可忽略性Ignorability：<span class="math inline">\(Y(1), Y(0) \perp T \mid X\)</span> ，意思是没有其他未观察到的混淆变量存在；</p></li>
<li><p>重叠Overlap： <span class="math inline">\(0&lt;\pi(x)&lt;1\)</span> ，即干预的施加是不确定的，存在倾向性。</p></li>
</ul>
<p><strong>ITE的基本框架</strong></p>
<ol style="list-style-type: decimal">
<li>样本数据集定义</li>
</ol>
<p>令观察样本为<span class="math inline">\(D=\left\{y_i, x_i, w_i\right\}_{i=1}^n\)</span>：表示包含<span class="math inline">\(n\)</span>个样本的数据集，每个样本包含三个核心变量：
<span class="math inline">\(\mathrm{y}, \mathrm{x}, \mathrm{w}\)</span> 分别表示效果标签、特征、是否被干预。每一个样本的 <span class="math inline">\(y_i \in\{0,1\}\)</span> ，表示binary的 outcome；每一个样本的 <span class="math inline">\(w_i \in\{0,1\}\)</span> 表示binary的treatment，当 <span class="math inline">\(w_i=1\)</span> 表示有干预，当 <span class="math inline">\(w_i=0\)</span> 表示无干预。</p>
<p>被干预的倾向性得分估计表示为 <span class="math inline">\(\pi(x)=P(W=1 \mid X=x)\)</span> 。
<span class="math inline">\(T=\left\{i: w_i=1\right\}\)</span>:干预样本的集和 <span class="math inline">\(C=\left\{i: w_i=0\right\}\)</span> ：对照组的样本集和。</p>
<p>2．倾向性得分（Propensity Score）</p>
<ul>
<li><p>定义：<span class="math inline">\(\pi(x)=P(W=1 \mid X=x)\)</span> ，表示在给定特征 <span class="math inline">\(x\)</span> 的条件下，样本接受干预（ <span class="math inline">\(W=1\)</span> ）的概率。</p></li>
<li><p>作用：倾向性得分是因果推断中的重要工具，常用于平衡实验组和对照组的特征分布，减少混杂偏倚（confounding bias）。</p></li>
</ul>
<p>3．干预效应的核心定义</p>
<p>要估计干预对结果的影响，需明确两个条件期望：</p>
<ul>
<li><p>干预组响应（TR）：<span class="math inline">\(\mu_1(x)=\mathbb{E}(Y \mid W=1, X=x)\)</span>表示＂在特征为 <span class="math inline">\(x\)</span> 且接受干预（ <span class="math inline">\(W=1\)</span> ）的条件下，结果 <span class="math inline">\(Y\)</span> 的期望＂（即特征为 <span class="math inline">\(x\)</span> 的样本接受干预后的平均结果）。</p></li>
<li><p>对照组响应（CR）：<span class="math inline">\(\mu_0(x)=\mathbb{E}(Y \mid W=0, X=x)\)</span>表示＂在特征为 <span class="math inline">\(x\)</span> 且未接受干预（ <span class="math inline">\(W=0\)</span> ）的条件下，结果 <span class="math inline">\(Y\)</span> 的期望＂（即特征为 <span class="math inline">\(x\)</span> 的样本未接受干预后的平均结果）。</p></li>
</ul>
<p>4．个体处理效应（ITE）的估计</p>
<ul>
<li>ITE 的定义：<span class="math inline">\(\tau(x)=\mu_1(x)-\mu_0(x)\)</span>表示对于特征为 <span class="math inline">\(x\)</span> 的个体，接受干预与未接受干预的结果差异，即＂干预对该个体的净效应＂。</li>
<li>估计思路：
1．通过建模分别估计 <span class="math inline">\(\mu_1(x)\)</span> 和 <span class="math inline">\(\mu_0(x)\)</span> ，得到估计值 <span class="math inline">\(\hat{\mu}_1(x)\)</span> 和<span class="math inline">\(\hat{\mu}_0(x)\)</span>（例如用回归模型分别拟合实验组和对照组的结果与特征的关系）。</li>
</ul>
<p>2．计算估计的 ITE：<span class="math inline">\(\hat{\tau}(x)=\hat{\mu}_1(x)-\hat{\mu}_0(x)\)</span> ，即通过两个条件期望的估计值之差，近似个体的真实干预效应。</p>
<p><strong>ESN结构</strong></p>
<p>如fig1 中的a图</p>
<p>ESN（Entire Space Network）通过定义两个关键概率来实现全空间建模，分别是 Entire Space Treated Response（ESTR）和 Entire Space Control Response （ESCR）。其中，ESTR 表示为 <span class="math inline">\(P(Y, W=1 \mid X)\)</span> ，即给定特征 <span class="math inline">\(X\)</span> 时，有干预且结果为 <span class="math inline">\(Y\)</span> 的联合概率，若 <span class="math inline">\(Y\)</span> 表示是否转化，ESTR 就是有干预且转化的概率； ESCR 表示为 <span class="math inline">\(P(Y, W=0 \mid X)\)</span> ，即给定特征 <span class="math inline">\(X\)</span> 时，无干预且结果为 <span class="math inline">\(Y\)</span> 的联合概率，若 <span class="math inline">\(Y\)</span> 表示是否转化， ESCR 就是无干预且转化的概率。</p>
<p>和Two model类型的模型将实验组和对照组样本分别建模为两个模型不同，ESN受多目标模型的启发，通过共享层对不同的数据提取Embedding。然后对于每个数据计算倾向程度分Propensity Score，对实验组数据进入干预分支得到ESTR，对于对照组数据进入对照分支得到ESCR。</p>
<p>基于此，ESN 通过拟合 ESTR、ESCR 以及倾向性 <span class="math inline">\(\pi\)</span> 的观测标签得到相应损失函数，分别是倾向损失 <span class="math inline">\(L_\pi\)</span> 、ESTR损失 <span class="math inline">\(L_{ESTR}\)</span> 和 ESCR 损失 <span class="math inline">\(L_{ESCR}\)</span> 。其中， <span class="math inline">\(L_\pi=\frac{1}{n} \sum_i l\left(t_i, \hat{\pi}\left(x_i\right)\right)\)</span> ，用于拟合倾向性得分； <span class="math inline">\(L_{\text {ESTR }}=\frac{1}{n} \sum_i l\left(y_i \&amp; w_i, \hat{\mu}_1\left(x_i\right) \cdot \hat{\pi}\left(x_i\right)\right)\)</span> ，用于拟合有干预且结果为 <span class="math inline">\(Y\)</span> 的情况<span class="math inline">\(L_{E S C R}=\frac{1}{n} \sum_i l\left(y_i \&amp;\left(1-w_i\right), \hat{\mu}_0\left(x_i\right) \cdot\left(1-\hat{\pi}\left(x_i\right)\right)\right)\)</span>:用于拟合无干预且结果为<span class="math inline">\(Y\)</span>的情况</p>
<p>合并后得到：<span class="math inline">\(L_{E S N}=\alpha \cdot L_\pi+\beta_1 \cdot L_{E S T R}+\beta_0 \cdot L_{E S C R}\)</span></p>
<p><strong>Xnetwork</strong></p>
<p>如fig1中的图b</p>
<p>是基于X-learner改进得到的端到端学习方法，整体流程有点类似于将X-learner组合为一个端到端的学习方式，也存在交叉思路的设计。</p>
<p>通过共享层后，左右两个分支分别对干预组数据和对照组数据进行建模，中间的PTE
（Pseudo Treatment Effe）得到 <span class="math inline">\(\tau^{\prime}\)</span> 为干预带来的隐藏的效果，然后X－Network是怎么做交叉的才是其核心所在。</p>
<p>先定义Cross Treated Response 、 Cross Control Response 两个节点
Cross Treated Response <span class="math inline">\(\mu_1^{\prime}:=\mu_0+\tau^{\prime}\)</span>
Cross Control Response <span class="math inline">\(\mu_0^{\prime}:=\mu_1-\tau^{\prime}\)</span>
对于 Cross Treated Response，其实是把 <span class="math inline">\(\mu_0\)</span> 当作反事实（counterfactual）预测函数去预测（如果不干预会怎么样），然后加上 PTE Netwrok的 <span class="math inline">\(\tau^{\prime}\)</span> 获得有干预时的respond。</p>
<p>对于 Cross Control Response，则是把 <span class="math inline">\(\mu_1\)</span> 当作反事实（counterfactual）预测函数去预测 （如果干预会怎么样），然后减去 PTE Netwrok的 <span class="math inline">\(\tau^{\prime}\)</span> 以此得到无干预时的respond。</p>
<p>然后Cross Treated Response、Cross Control Response 分别对T、C数据集拟合，</p>
<p><span class="math display">\[
\begin{aligned}
L_{T R} &amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1\left(x_i\right)\right), \\
L_{C R} &amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0\left(x_i\right)\right), \\
L_{C r o s s T R} &amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_0\left(x_i\right)\right)+\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right),\right. \\
L_{C r o s s C R} &amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_1\left(x_i\right)\right)-\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right.
\end{aligned}
\]</span>
逐一解释每个损失的含义</p>
<p>1．基础损失（直接响应预测）</p>
<p><span class="math display">\[
L_{T R}=\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1\left(x_i\right)\right)
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(T\)</span> 是处理组（Treatment Group）样本集合，<span class="math inline">\(|T|\)</span> 是处理组样本数量。</li>
<li><span class="math inline">\(\hat{\mu}_1\left(x_i\right)\)</span> 是模型对处理组样本 <span class="math inline">\(x_i\)</span> 的响应预测（即干预后的结果）。</li>
<li><span class="math inline">\(l(\cdot, \cdot)\)</span> 是损失函数（如分类用交叉嫡、回归用 MSE），衡量预测值与真实标签 <span class="math inline">\(y_i\)</span> 的差异。</li>
<li>作用：直接学习＂处理组特征 <span class="math inline">\(x_i \rightarrow\)</span> 处理后结果 <span class="math inline">\(y_i\)</span>＂的映射。</li>
</ul>
<p><span class="math display">\[
L_{C R}=\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0\left(x_i\right)\right)
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(C\)</span> 是对照组（Control Group）样本集合，<span class="math inline">\(|C|\)</span> 是对照组样本数量。
<span class="math inline">\(\hat{\mu}_0\left(x_i\right)\)</span> 是模型对对照组样本 <span class="math inline">\(x_i\)</span> 的响应预测（即未干预时的结果）。
作用：直接学习＂对照组特征 <span class="math inline">\(x_i \rightarrow\)</span> 未干预结果 <span class="math inline">\(y_i\)</span>＂的映射。</li>
</ul>
<p>2．交叉损失（反事实预测）</p>
<p><span class="math display">\[
\begin{aligned}
&amp; L_{\text {CrossTR }}=\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_0\left(x_i\right)\right)+\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right)
\end{aligned}
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(\hat{\mu}_1^{\prime}\left(x_i\right)\)</span> 是反事实预测：用对照组的预测结果 <span class="math inline">\(\hat{\mu}_0\left(x_i\right)\)</span> 加上＂处理效应＂<span class="math inline">\(\hat{\tau}^{\prime}\left(x_i\right)\)</span> ，模拟处理组的结果。</li>
<li><span class="math inline">\(\sigma(\cdot)\)</span> 是激活函数（如分类用 Sigmoid，回归可能不用），<span class="math inline">\(\sigma^{-1}(\cdot)\)</span> 是其逆函数（如 Sigmoid 逆是 Logit 变换）。</li>
<li>作用：让模型学习＂如果对照组样本 <span class="math inline">\(x_i\)</span> 被处理，结果会怎样＂（反事实推理）。通过强制模型用 <span class="math inline">\(\hat{\mu}_0+\tau^{\prime}\)</span> 预测处理组结果，约束处理效应 <span class="math inline">\(\tau^{\prime}\)</span> 的合理性。</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; L_{\text {CrossCR }}=\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_1\left(x_i\right)\right)-\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right)
\end{aligned}
\]</span></p>
<ul>
<li><p>含义：</p></li>
<li><p><span class="math inline">\(\hat{\mu}_0^{\prime}\left(x_i\right)\)</span> 是反事实预测：用处理组的预测结果 <span class="math inline">\(\hat{\mu}_1\left(x_i\right)\)</span> 减去＂处理效应＂<span class="math inline">\(\hat{\tau}^{\prime}\left(x_i\right)\)</span> ，模拟对照组的结果。</p></li>
</ul>
<p>作用：让模型学习＂如果处理组样本 <span class="math inline">\(x_i\)</span> 未被处理，结果会怎样＂（反事实推理）。通过 <span class="math inline">\(\hat{\mu}_1-\tau^{\prime}\)</span> 约束处理效应 <span class="math inline">\(\tau^{\prime}\)</span> ，确保效应的一致性。</p>
<p>核心逻辑：反事实约束</p>
<ul>
<li><p>处理效应 <span class="math inline">\(\tau(x)\)</span> 的定义是：<span class="math inline">\(\tau(x)=\mu_1(x)-\mu_0(x)\)</span>（处理后结果－未处理结果）。</p></li>
<li><p>CrossTR／CrossCR 通过反事实假设，强制模型满足：<span class="math inline">\(\mu_1(x)=\mu_0(x)+\tau(x)\)</span>（处理组结果 <span class="math inline">\(=\)</span> 对照组结果 + 处理效应）</p></li>
</ul>
<p><span class="math inline">\(\mu_0(x)=\mu_1(x)-\tau(x)(\)</span> 对照组结果 <span class="math inline">\(=\)</span> 处理组结果 - 处理效应 <span class="math inline">\()\)</span>
－这种约束让模型学习到的处理效应 <span class="math inline">\(\tau(x)\)</span> 更可靠，避免效应预测与直接响应预测矛盾。</p>
<p>总结</p>
<ul>
<li><p>基础损失（ <span class="math inline">\(L_{T R}, L_{C R}\)</span> ）：直接学习＂处理／对照组特征 <span class="math inline">\(\rightarrow\)</span> 结果＂的映射。</p></li>
<li><p>交叉损失（ <span class="math inline">\(L_{\text {CrossTR }}, L_{\text {CrossCR }}\)</span> ）：通过反事实推理，约束＂处理效应 <span class="math inline">\(\tau^{\prime \prime}\)</span> 的合理性，让模型同时满足：</p></li>
<li><p>处理组结果 <span class="math inline">\(=\)</span> 对照组结果 + 处理效应</p></li>
<li><p>对照组结果 <span class="math inline">\(=\)</span> 处理组结果－处理效应</p></li>
<li><p>这种设计是 X－learner 的核心思想，通过反事实约束提升 <span class="math inline">\(5 \vee\)</span> 立预测的准确性，常用于因果推断、uplift modeling 等场景。</p></li>
</ul>
<p>公式中 <span class="math inline">\(\sigma^{-1}\)</span> 表示sigmoid的逆函数，实际上 Cross Treated Response、Cross Control Response 会对 <span class="math inline">\(\tau^{\prime}\)</span> 在 logit 层面做加减，这么做的优点有二：</p>
<p>1．避免数值范围截断：sigmoid 函数的输出范围固定在 <span class="math inline">\((0,1)\)</span> 之间，若直接在概率层面对 <span class="math inline">\(\tau^{\prime}\)</span> 进行加减，可能导致结果超出合理范围（如小于 0 或大于 1），需要额外的截断处理，这会丟失信息或引入偏差。而 logit 层面（ <span class="math inline">\(\sigma^{-1}\)</span> 的输出）是实数域，加减操作不会受边界限制，无需截断。</p>
<p>2．增强小效应信号的捕捉能力：sigmoid 函数的特性是两端斜率极低（接近 0 ），中间区域斜率较高。这意味着在概率接近 0 或 1 时，纵轴（概率）的微小变化对应横轴（logit）的较大变化。通过在 logit 层面操作 <span class="math inline">\(\tau^{\prime}\)</span> ，即使是很小的效应信号，也能通过 logit 空间的显著变化被模型捕捉，从而提升对微弱干预效果的识别能力。</p>
<p><strong>ESN</strong></p>
<p>整体结构如fig1中的图c
DESCN就是将上面ESN（处理Treatment bias的能力）和 X－network（处理Sample imbalance的能力）结合，结构如图（c），损失函数如下：</p>
<p><span class="math display">\[
\begin{aligned}
L_{D E S C N} &amp; =L_{E S N}+\gamma_1 \cdot L_{C r o s s T R}+\gamma_0 \cdot L_{\mathrm{CrossCR}} \\
&amp; =\alpha \cdot L_\pi+\beta_1 \cdot L_{E S T R}+\beta_0 \cdot L_{E S C R} \\
&amp; +\gamma_1 \cdot L_{C r o s s T R}+\gamma_0 \cdot L_{C r o s s C R}
\end{aligned}
\]</span>
效果对比
<img src="refs/descn-res.png" />
# 多目标多场景</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="基础定义.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="treatment类型.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": false,
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
