<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 模型 | 因果模型学习笔记</title>
  <meta name="description" content="3 模型 | 因果模型学习笔记" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="3 模型 | 因果模型学习笔记" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 模型 | 因果模型学习笔记" />
  
  
  

<meta name="author" content="高文欣" />


<meta name="date" content="2025-08-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="uplift-model与response-model区别.html"/>
<link rel="next" href="数据收集.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="le_uplift.html"><a href="le_uplift.html"><i class="fa fa-check"></i><b>1</b> le_uplift</a></li>
<li class="chapter" data-level="2" data-path=""><a href="#uplift-model%E4%B8%8Eresponse-model%E5%8C%BA%E5%88%AB"><i class="fa fa-check"></i><b>2</b> uplift-model与response-model区别</a></li>
<li class="chapter" data-level="3" data-path=""><a href="#%E6%A8%A1%E5%9E%8B"><i class="fa fa-check"></i><b>3</b> 模型</a>
<ul>
<li class="chapter" data-level="3.1" data-path="模型.html"><a href="模型.html"><i class="fa fa-check"></i><b>3.1</b> meta-learner</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="模型.html"><a href="模型.html#slearner"><i class="fa fa-check"></i><b>3.1.1</b> Slearner</a></li>
<li class="chapter" data-level="3.1.2" data-path="模型.html"><a href="模型.html#tlearner"><i class="fa fa-check"></i><b>3.1.2</b> TLearner</a></li>
<li class="chapter" data-level="3.1.3" data-path="模型.html"><a href="模型.html#xlearner"><i class="fa fa-check"></i><b>3.1.3</b> XLearner</a></li>
<li class="chapter" data-level="3.1.4" data-path="模型.html"><a href="模型.html#rlearner"><i class="fa fa-check"></i><b>3.1.4</b> RLearner</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#%E5%9B%A0%E6%9E%9C%E6%A0%91"><i class="fa fa-check"></i><b>3.2</b> 因果树</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="模型.html"><a href="模型.html#casusaltree"><i class="fa fa-check"></i><b>3.2.1</b> Casusaltree</a></li>
<li class="chapter" data-level="3.2.2" data-path="模型.html"><a href="模型.html#casusalforest"><i class="fa fa-check"></i><b>3.2.2</b> CasusalForest</a></li>
<li class="chapter" data-level="3.2.3" data-path="模型.html"><a href="模型.html#dml"><i class="fa fa-check"></i><b>3.2.3</b> DML</a></li>
<li class="chapter" data-level="3.2.4" data-path="模型.html"><a href="模型.html#drl"><i class="fa fa-check"></i><b>3.2.4</b> DRL</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#%E6%B7%B1%E5%BA%A6%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B"><i class="fa fa-check"></i><b>3.3</b> 深度因果模型</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="模型.html"><a href="模型.html#tarnet"><i class="fa fa-check"></i><b>3.3.1</b> TARNET</a></li>
<li class="chapter" data-level="3.3.2" data-path="模型.html"><a href="模型.html#dragonnet"><i class="fa fa-check"></i><b>3.3.2</b> Dragonnet</a></li>
<li class="chapter" data-level="3.3.3" data-path="模型.html"><a href="模型.html#efin"><i class="fa fa-check"></i><b>3.3.3</b> EFIN</a></li>
<li class="chapter" data-level="3.3.4" data-path="模型.html"><a href="模型.html#descn"><i class="fa fa-check"></i><b>3.3.4</b> DESCN</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><i class="fa fa-check"></i><b>4</b> 数据收集</a></li>
<li class="chapter" data-level="5" data-path=""><a href="#%E5%8E%BB%E5%81%8F%E5%B7%AE"><i class="fa fa-check"></i><b>5</b> 去偏差</a></li>
<li class="chapter" data-level="6" data-path=""><a href="#%E8%AF%84%E4%BC%B0"><i class="fa fa-check"></i><b>6</b> 评估</a>
<ul>
<li class="chapter" data-level="6.1" data-path=""><a href="#%E5%BA%8F%E5%87%86"><i class="fa fa-check"></i><b>6.1</b> 序准</a></li>
<li class="chapter" data-level="6.2" data-path=""><a href="#%E5%80%BC%E5%87%86"><i class="fa fa-check"></i><b>6.2</b> 值准</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path=""><a href="#%E9%A2%84%E7%AE%97%E5%88%86%E9%85%8D"><i class="fa fa-check"></i><b>7</b> 预算分配</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">因果模型学习笔记</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="模型" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> 模型<a href="#%E6%A8%A1%E5%9E%8B" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="meta-learner" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> meta-learner<a href="模型.html#meta-learner" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="slearner" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Slearner<a href="模型.html#slearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>s 顾名思义：single model，就是一个模型都搞定
这个模型最简单，直接把treatment作为特征放进模型来预测。首先我们把 T 作为特征一起放进机器学习模型的特征， Y 是目标，然后训练一个有监督的模型 <span class="math inline">\(\mu(x)=E[Y \mid X=x, T=t]\)</span> 。然后我们改变 T 的值，就可以得到两个不同的结果，再一相减就好了：
<span class="math display">\[\hat{\tau}(x)=\hat{\mu}(x, T=1)-\hat{\mu}(x, T=0)\]</span>
<img src="refs/slearner.jpg" />
优点：简单</p>
<p>缺点：
- 1. 本质上还是不是对 uplift 直接进行建模，因此从效果上来说还是有提升空间。
- 2. S-Learner 倾向于将干预效果趋近于0，尤其是特征的维度非常大的时候，模型在训练的过程中极容易忽略这个干预变量。同时模型中正则化的引入，也会带来干预变量的效果稀释，正则化越强，该问题就会越大。</p>
<p>代码</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"></code></pre></div>
</div>
<div id="tlearner" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> TLearner<a href="模型.html#tlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>T 顾名思义：two model 需要用俩模型（可以是任何的response模型）</p>
<p>对于 treatment 组的样本和 control 组的样本分别独立训练一个响应模型 <span class="math inline">\(\hat{\mu}_1(x)\)</span>和 <span class="math inline">\(\hat{\mu}_0(x)\)</span> 。其中 <span class="math inline">\(\hat{\mu}_1(x)\)</span> 用于拟合在施加干预的情况下响应目标 <span class="math inline">\(Y\)</span> 与特征 <span class="math inline">\(X\)</span> 的关系，即 <span class="math inline">\(\mu_1(x)=E[Y \mid W=1, X=x]\)</span> ；而 <span class="math inline">\(\hat{\mu}_0(x)\)</span> 用于拟合在未施加干预的情况下响应目标 <span class="math inline">\(Y\)</span>与特征 <span class="math inline">\(X\)</span> 的关系，即 <span class="math inline">\(\mu_0(x)=E[Y \mid W=0, X=x]\)</span> 。</p>
<p>之后拿训练好的两个模型对于同一个样本 <span class="math inline">\(x\)</span> 的预测结果做差，就得到了 uplift 结果。</p>
<p>最终得到的 T－Learner 表达式为 <span class="math inline">\(\hat{\tau}_T(x)=\hat{\mu}_1(x)-\hat{\mu}_0(x)\)</span>
下面是 T－Learner 算法的示意图（参考 Causal Inference for The Brave and True 这本书，因此该图部分 notation 与上文存在差异：图中的 T 对应上文的干预 <span class="math inline">\(W\)</span> ；图中的 M1和 M 0 对应上文的响应模型 <span class="math inline">\(\hat{\mu}_1\)</span> 和 <span class="math inline">\(\hat{\mu}_0\)</span> ）<a href="https://zhuanlan.zhihu.com/p/682079211">参考</a></p>
<div class="float">
<img src="refs/tlearner.jpg" alt="tlearner" />
<div class="figcaption">tlearner</div>
</div>
<p>优点：可以灵活地使用已有的机器学习方法。对干预和非干预样本分别建模，充分考虑了干预因素的影响。</p>
<p>缺点：
- 1. T-Learner 并不是直接对 uplift 进行建模，因此对 uplift 的识别能力有限。
- 2. 考虑到其基于两个独立训练的模型进行二次处理后来对 uplift 进行预测，很容易产生两个独立模型的误差累积的问题。</p>
</div>
<div id="xlearner" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> XLearner<a href="模型.html#xlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>1．参考T-learner的思路，先对于 <span class="math inline">\(T=0\)</span> 的control组和 <span class="math inline">\(T=1\)</span> 的treatment组分别学习一个有监督的模型。
<img src="refs/xlearner2.jpg" alt="xlearner" /></p>
<p><span class="math display">\[
\begin{aligned}
&amp; \mu_0(x)=E\left[Y^0 \mid X=x\right] \\
&amp; \mu_1(x)=E\left[Y^1 \mid X=x\right]
\end{aligned}
\]</span></p>
<p>2．然后对于 <span class="math inline">\(\mathrm{T}=1\)</span> 的样本和 <span class="math inline">\(\mathrm{T}=0\)</span> 的样本，分别使用 <span class="math inline">\(\mu_0(x)\)</span> 和 <span class="math inline">\(\mu_1(x)\)</span> 预测一个 <span class="math inline">\(\hat{\mu}^0\left(X^1\right)\)</span> 和 <span class="math inline">\(\hat{\mu}^1\left(X^0\right)\)</span> ，这里 <span class="math inline">\(X^0, X^1\)</span> 分别是 <span class="math inline">\(\mathrm{T}=0, \mathrm{~T}=1\)</span> 组 的样本，这步就是就是获得一个反事实的结果 （比如对于某一个 <span class="math inline">\(\mathrm{T}=1\)</span> 的样本 <span class="math inline">\(X_i^1\)</span> ，事实结果是 <span class="math inline">\(Y_i^1\)</span> ，反事实结果是 <span class="math inline">\(\hat{\mu}^0\left(X_i^1\right)\)</span> 。于是我们就可以对于control组和treatment组分别计算difference <span class="math inline">\(D_i^0\)</span> 和 <span class="math inline">\(D_i^1\)</span> ：</p>
<p><span class="math display">\[
\begin{aligned}
D_i^1 &amp; =Y_i^1-\hat{\mu}^0\left(X_i^1\right) \\
D_i^0 &amp; =\hat{\mu}^1\left(X_i^0\right)-Y_i^0
\end{aligned}
\]</span></p>
<p>3．最终对于一个新样本的CATE就是这两个的加权平均，权重是什么呢？一般是propensity score <span class="math inline">\(g(x)=P(T=1 \mid X=x)\)</span> ，这个可以通过一个LR模型或者任何二分类模型得到得到。最后新样本的的CATE如下：<span class="math inline">\(\hat{\tau}(x)=g(x) \hat{\tau}_0(x)+(1-g(x)) \hat{\tau}_1(x)\)</span></p>
<p><img src="refs/xlearner.jpg" alt="xlearner" />
下面是 X－Learner 算法的示意图（同样参考 Causal Inference for The Brave and True 这本书，图中 notation 与上文存在差异：图中的 T 对应上文的干预 <span class="math inline">\(W\)</span> ；图中的 M0 和 M1对应前面的模型 <span class="math inline">\(\hat{\mu}_0\)</span> 和 <span class="math inline">\(\hat{\mu}_1\)</span> ；图中的 MTAU0 和 MTAU1 对应前面的模型 <span class="math inline">\(\hat{\tau}_0\)</span> 和 <span class="math inline">\(\hat{\tau}_1\)</span> ）<a href="https://zhuanlan.zhihu.com/p/682079211">参考</a></p>
</div>
<div id="rlearner" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> RLearner<a href="模型.html#rlearner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="因果树" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> 因果树<a href="#%E5%9B%A0%E6%9E%9C%E6%A0%91" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="casusaltree" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Casusaltree<a href="模型.html#casusaltree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="casusalforest" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> CasusalForest<a href="模型.html#casusalforest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="dml" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> DML<a href="模型.html#dml" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="drl" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> DRL<a href="模型.html#drl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="深度因果模型" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> 深度因果模型<a href="#%E6%B7%B1%E5%BA%A6%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="tarnet" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> TARNET<a href="模型.html#tarnet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>深度因果模型的基础网络，很多后续的网络都是在此基础上拓展的.为了让神经网络学习到的表示相近，Shalit et al．（2017）提出了Treatment Agnostic Regression Network（TARNet）。它的思想很简单，神经网络整体为双头结构，然后像 multi－task一样的共享层去学习共有的信息表征 <span class="math inline">\(\phi(x)\)</span> ，再利用该共享表征去分别学习treated组和control的outcome结果。共享层其实就会在treated和control组的信息中进行tread off，从而达到一种balanced。
简言之：TARNet前半部分用的是整个数据集，但是在后两个分支分别用的是T=0和T=1的分组数据各自训练所属的分支。</p>
<p><img src="refs/tarnet.png" alt="tarnet" />
图a是T-Learner，图b是tarnet，其中tarnet在2元treatment的loss如下：<a href="https://zhuanlan.zhihu.com/p/603499723">参考</a>
<span class="math display">\[
\underset{h, \Phi}{\arg \min } \frac{1}{N} \sum_{i=1}^N \operatorname{MSE}(Y_i\left(T_i\right), \underbrace{h\left(\Phi\left(X_i\right), T_i\right)}_{\hat{Y}_i\left(T_i\right)})+\lambda \underbrace{\mathcal{R}(h)}_{L_2}
\]</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="模型.html#cb2-1" tabindex="-1"></a>def <span class="fu">regression_loss</span>(concat_true, concat_pred)<span class="sc">:</span></span>
<span id="cb2-2"><a href="模型.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="模型.html#cb2-3" tabindex="-1"></a>    y_true <span class="ot">=</span> concat_true[<span class="sc">:</span>, <span class="dv">0</span>]</span>
<span id="cb2-4"><a href="模型.html#cb2-4" tabindex="-1"></a>    t_true <span class="ot">=</span> concat_true[<span class="sc">:</span>, <span class="dv">1</span>]</span>
<span id="cb2-5"><a href="模型.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="模型.html#cb2-6" tabindex="-1"></a>    y0_pred <span class="ot">=</span> concat_pred[<span class="sc">:</span>, <span class="dv">0</span>]</span>
<span id="cb2-7"><a href="模型.html#cb2-7" tabindex="-1"></a>    y1_pred <span class="ot">=</span> concat_pred[<span class="sc">:</span>, <span class="dv">1</span>]</span>
<span id="cb2-8"><a href="模型.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="模型.html#cb2-9" tabindex="-1"></a>    loss0 <span class="ot">=</span> <span class="fu">tf.reduce_sum</span>((<span class="fl">1.</span> <span class="sc">-</span> t_true) <span class="sc">*</span> <span class="fu">tf.square</span>(y_true <span class="sc">-</span> y0_pred))</span>
<span id="cb2-10"><a href="模型.html#cb2-10" tabindex="-1"></a>    loss1 <span class="ot">=</span> <span class="fu">tf.reduce_sum</span>(t_true <span class="sc">*</span> <span class="fu">tf.square</span>(y_true <span class="sc">-</span> y1_pred))</span>
<span id="cb2-11"><a href="模型.html#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="模型.html#cb2-12" tabindex="-1"></a>    return loss0 <span class="sc">+</span> loss1</span></code></pre></div>
</div>
<div id="dragonnet" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Dragonnet<a href="模型.html#dragonnet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="efin" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> EFIN<a href="模型.html#efin" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="descn" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> DESCN<a href="模型.html#descn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>模型整体有两部分结构组成ESN网络+Xnetwork网络</p>
<div class="float">
<img src="refs/descn.png" alt="descn" />
<div class="figcaption">descn</div>
</div>
<p>解决的问题</p>
<ul>
<li><p>干预偏差问题 Treatment Bias： 即被干预Treatment组（实验组）和未干预Control 组（对照组）的分布是不同的。</p></li>
<li><p>样本不均衡问题 Sample Imbalance：干预的Treatment 组和未干预的Control 组样本差异大，样本不平衡。</p></li>
</ul>
<p>问题根源：Treatment Bias 和 Sample Imbalance 都源于非随机的观测数据。</p>
<p>Treatment Bias 是因为处理分配（W）与协变量也就是特这（X）和潜在结果（Y(0), Y(1)）不独立，即存在混淆变量。</p>
<p>Sample Imbalance 虽然有时是策略本身导致的（如只对1%的用户发券），但在非随机环境下，这种数量上的不平衡会加剧模型学习的难度。</p>
<p>理论上来说，RCT实验数据是可以一定程度避免这俩问题的。
本文方法的三个假设：</p>
<ul>
<li><p>一致性Consistency：<span class="math inline">\(y_i=y_i\left(w_i\right)\)</span> ，观测到的 <span class="math inline">\(y_i\)</span> 和 潜在的 <span class="math inline">\(y_i\left(w_i\right)\)</span> 是一致的，即干预对结果的影响是稳定的。</p></li>
<li><p>可忽略性Ignorability：<span class="math inline">\(Y(1), Y(0) \perp T \mid X\)</span> ，意思是没有其他未观察到的混淆变量存在；</p></li>
<li><p>重叠Overlap： <span class="math inline">\(0&lt;\pi(x)&lt;1\)</span> ，即干预的施加是不确定的，存在倾向性。</p></li>
</ul>
<p><strong>ITE的基本框架</strong></p>
<ol style="list-style-type: decimal">
<li>样本数据集定义</li>
</ol>
<p>令观察样本为<span class="math inline">\(D=\left\{y_i, x_i, w_i\right\}_{i=1}^n\)</span>：表示包含<span class="math inline">\(n\)</span>个样本的数据集，每个样本包含三个核心变量：
<span class="math inline">\(\mathrm{y}, \mathrm{x}, \mathrm{w}\)</span> 分别表示效果标签、特征、是否被干预。每一个样本的 <span class="math inline">\(y_i \in\{0,1\}\)</span> ，表示binary的 outcome；每一个样本的 <span class="math inline">\(w_i \in\{0,1\}\)</span> 表示binary的treatment，当 <span class="math inline">\(w_i=1\)</span> 表示有干预，当 <span class="math inline">\(w_i=0\)</span> 表示无干预。</p>
<p>被干预的倾向性得分估计表示为 <span class="math inline">\(\pi(x)=P(W=1 \mid X=x)\)</span> 。
<span class="math inline">\(T=\left\{i: w_i=1\right\}\)</span>:干预样本的集和 <span class="math inline">\(C=\left\{i: w_i=0\right\}\)</span> ：对照组的样本集和。</p>
<p>2．倾向性得分（Propensity Score）</p>
<ul>
<li><p>定义：<span class="math inline">\(\pi(x)=P(W=1 \mid X=x)\)</span> ，表示在给定特征 <span class="math inline">\(x\)</span> 的条件下，样本接受干预（ <span class="math inline">\(W=1\)</span> ）的概率。</p></li>
<li><p>作用：倾向性得分是因果推断中的重要工具，常用于平衡实验组和对照组的特征分布，减少混杂偏倚（confounding bias）。</p></li>
</ul>
<p>3．干预效应的核心定义</p>
<p>要估计干预对结果的影响，需明确两个条件期望：</p>
<ul>
<li><p>干预组响应（TR）：<span class="math inline">\(\mu_1(x)=\mathbb{E}(Y \mid W=1, X=x)\)</span>表示＂在特征为 <span class="math inline">\(x\)</span> 且接受干预（ <span class="math inline">\(W=1\)</span> ）的条件下，结果 <span class="math inline">\(Y\)</span> 的期望＂（即特征为 <span class="math inline">\(x\)</span> 的样本接受干预后的平均结果）。</p></li>
<li><p>对照组响应（CR）：<span class="math inline">\(\mu_0(x)=\mathbb{E}(Y \mid W=0, X=x)\)</span>表示＂在特征为 <span class="math inline">\(x\)</span> 且未接受干预（ <span class="math inline">\(W=0\)</span> ）的条件下，结果 <span class="math inline">\(Y\)</span> 的期望＂（即特征为 <span class="math inline">\(x\)</span> 的样本未接受干预后的平均结果）。</p></li>
</ul>
<p>4．个体处理效应（ITE）的估计</p>
<ul>
<li>ITE 的定义：<span class="math inline">\(\tau(x)=\mu_1(x)-\mu_0(x)\)</span>表示对于特征为 <span class="math inline">\(x\)</span> 的个体，接受干预与未接受干预的结果差异，即＂干预对该个体的净效应＂。</li>
<li>估计思路：
1．通过建模分别估计 <span class="math inline">\(\mu_1(x)\)</span> 和 <span class="math inline">\(\mu_0(x)\)</span> ，得到估计值 <span class="math inline">\(\hat{\mu}_1(x)\)</span> 和<span class="math inline">\(\hat{\mu}_0(x)\)</span>（例如用回归模型分别拟合实验组和对照组的结果与特征的关系）。</li>
</ul>
<p>2．计算估计的 ITE：<span class="math inline">\(\hat{\tau}(x)=\hat{\mu}_1(x)-\hat{\mu}_0(x)\)</span> ，即通过两个条件期望的估计值之差，近似个体的真实干预效应。</p>
<p><strong>ESN结构</strong></p>
<p>如fig1 中的a图</p>
<p>ESN（Entire Space Network）通过定义两个关键概率来实现全空间建模，分别是 Entire Space Treated Response（ESTR）和 Entire Space Control Response （ESCR）。其中，ESTR 表示为 <span class="math inline">\(P(Y, W=1 \mid X)\)</span> ，即给定特征 <span class="math inline">\(X\)</span> 时，有干预且结果为 <span class="math inline">\(Y\)</span> 的联合概率，若 <span class="math inline">\(Y\)</span> 表示是否转化，ESTR 就是有干预且转化的概率； ESCR 表示为 <span class="math inline">\(P(Y, W=0 \mid X)\)</span> ，即给定特征 <span class="math inline">\(X\)</span> 时，无干预且结果为 <span class="math inline">\(Y\)</span> 的联合概率，若 <span class="math inline">\(Y\)</span> 表示是否转化， ESCR 就是无干预且转化的概率。</p>
<p>和Two model类型的模型将实验组和对照组样本分别建模为两个模型不同，ESN受多目标模型的启发，通过共享层对不同的数据提取Embedding。然后对于每个数据计算倾向程度分Propensity Score，对实验组数据进入干预分支得到ESTR，对于对照组数据进入对照分支得到ESCR。</p>
<p>基于此，ESN 通过拟合 ESTR、ESCR 以及倾向性 <span class="math inline">\(\pi\)</span> 的观测标签得到相应损失函数，分别是倾向损失 <span class="math inline">\(L_\pi\)</span> 、ESTR损失 <span class="math inline">\(L_{ESTR}\)</span> 和 ESCR 损失 <span class="math inline">\(L_{ESCR}\)</span> 。其中， <span class="math inline">\(L_\pi=\frac{1}{n} \sum_i l\left(t_i, \hat{\pi}\left(x_i\right)\right)\)</span> ，用于拟合倾向性得分； <span class="math inline">\(L_{\text {ESTR }}=\frac{1}{n} \sum_i l\left(y_i \&amp; w_i, \hat{\mu}_1\left(x_i\right) \cdot \hat{\pi}\left(x_i\right)\right)\)</span> ，用于拟合有干预且结果为 <span class="math inline">\(Y\)</span> 的情况<span class="math inline">\(L_{E S C R}=\frac{1}{n} \sum_i l\left(y_i \&amp;\left(1-w_i\right), \hat{\mu}_0\left(x_i\right) \cdot\left(1-\hat{\pi}\left(x_i\right)\right)\right)\)</span>:用于拟合无干预且结果为<span class="math inline">\(Y\)</span>的情况</p>
<p>合并后得到：<span class="math inline">\(L_{E S N}=\alpha \cdot L_\pi+\beta_1 \cdot L_{E S T R}+\beta_0 \cdot L_{E S C R}\)</span></p>
<p><strong>Xnetwork</strong></p>
<p>如fig1中的图b</p>
<p>是基于X-learner改进得到的端到端学习方法，整体流程有点类似于将X-learner组合为一个端到端的学习方式，也存在交叉思路的设计。</p>
<p>通过共享层后，左右两个分支分别对干预组数据和对照组数据进行建模，中间的PTE
（Pseudo Treatment Effe）得到 <span class="math inline">\(\tau^{\prime}\)</span> 为干预带来的隐藏的效果，然后X－Network是怎么做交叉的才是其核心所在。</p>
<p>先定义Cross Treated Response 、 Cross Control Response 两个节点
Cross Treated Response <span class="math inline">\(\mu_1^{\prime}:=\mu_0+\tau^{\prime}\)</span>
Cross Control Response <span class="math inline">\(\mu_0^{\prime}:=\mu_1-\tau^{\prime}\)</span>
对于 Cross Treated Response，其实是把 <span class="math inline">\(\mu_0\)</span> 当作反事实（counterfactual）预测函数去预测（如果不干预会怎么样），然后加上 PTE Netwrok的 <span class="math inline">\(\tau^{\prime}\)</span> 获得有干预时的respond。</p>
<p>对于 Cross Control Response，则是把 <span class="math inline">\(\mu_1\)</span> 当作反事实（counterfactual）预测函数去预测 （如果干预会怎么样），然后减去 PTE Netwrok的 <span class="math inline">\(\tau^{\prime}\)</span> 以此得到无干预时的respond。</p>
<p>然后Cross Treated Response、Cross Control Response 分别对T、C数据集拟合，</p>
<p><span class="math display">\[
\begin{aligned}
L_{T R} &amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1\left(x_i\right)\right), \\
L_{C R} &amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0\left(x_i\right)\right), \\
L_{C r o s s T R} &amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_0\left(x_i\right)\right)+\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right),\right. \\
L_{C r o s s C R} &amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_1\left(x_i\right)\right)-\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right.
\end{aligned}
\]</span>
逐一解释每个损失的含义</p>
<p>1．基础损失（直接响应预测）</p>
<p><span class="math display">\[
L_{T R}=\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1\left(x_i\right)\right)
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(T\)</span> 是处理组（Treatment Group）样本集合，<span class="math inline">\(|T|\)</span> 是处理组样本数量。</li>
<li><span class="math inline">\(\hat{\mu}_1\left(x_i\right)\)</span> 是模型对处理组样本 <span class="math inline">\(x_i\)</span> 的响应预测（即干预后的结果）。</li>
<li><span class="math inline">\(l(\cdot, \cdot)\)</span> 是损失函数（如分类用交叉嫡、回归用 MSE），衡量预测值与真实标签 <span class="math inline">\(y_i\)</span> 的差异。</li>
<li>作用：直接学习＂处理组特征 <span class="math inline">\(x_i \rightarrow\)</span> 处理后结果 <span class="math inline">\(y_i\)</span>＂的映射。</li>
</ul>
<p><span class="math display">\[
L_{C R}=\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0\left(x_i\right)\right)
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(C\)</span> 是对照组（Control Group）样本集合，<span class="math inline">\(|C|\)</span> 是对照组样本数量。
<span class="math inline">\(\hat{\mu}_0\left(x_i\right)\)</span> 是模型对对照组样本 <span class="math inline">\(x_i\)</span> 的响应预测（即未干预时的结果）。
作用：直接学习＂对照组特征 <span class="math inline">\(x_i \rightarrow\)</span> 未干预结果 <span class="math inline">\(y_i\)</span>＂的映射。</li>
</ul>
<p>2．交叉损失（反事实预测）</p>
<p><span class="math display">\[
\begin{aligned}
&amp; L_{\text {CrossTR }}=\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \hat{\mu}_1^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|T|} \sum_{i \in T} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_0\left(x_i\right)\right)+\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right)
\end{aligned}
\]</span></p>
<ul>
<li>含义：</li>
<li><span class="math inline">\(\hat{\mu}_1^{\prime}\left(x_i\right)\)</span> 是反事实预测：用对照组的预测结果 <span class="math inline">\(\hat{\mu}_0\left(x_i\right)\)</span> 加上＂处理效应＂<span class="math inline">\(\hat{\tau}^{\prime}\left(x_i\right)\)</span> ，模拟处理组的结果。</li>
<li><span class="math inline">\(\sigma(\cdot)\)</span> 是激活函数（如分类用 Sigmoid，回归可能不用），<span class="math inline">\(\sigma^{-1}(\cdot)\)</span> 是其逆函数（如 Sigmoid 逆是 Logit 变换）。</li>
<li>作用：让模型学习＂如果对照组样本 <span class="math inline">\(x_i\)</span> 被处理，结果会怎样＂（反事实推理）。通过强制模型用 <span class="math inline">\(\hat{\mu}_0+\tau^{\prime}\)</span> 预测处理组结果，约束处理效应 <span class="math inline">\(\tau^{\prime}\)</span> 的合理性。</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; L_{\text {CrossCR }}=\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \hat{\mu}_0^{\prime}\left(x_i\right)\right) \\
&amp; =\frac{1}{|C|} \sum_{i \in C} l\left(y_i, \sigma\left(\sigma^{-1}\left(\hat{\mu}_1\left(x_i\right)\right)-\sigma^{-1}\left(\hat{\tau}^{\prime}\left(x_i\right)\right)\right)\right)
\end{aligned}
\]</span></p>
<ul>
<li><p>含义：</p></li>
<li><p><span class="math inline">\(\hat{\mu}_0^{\prime}\left(x_i\right)\)</span> 是反事实预测：用处理组的预测结果 <span class="math inline">\(\hat{\mu}_1\left(x_i\right)\)</span> 减去＂处理效应＂<span class="math inline">\(\hat{\tau}^{\prime}\left(x_i\right)\)</span> ，模拟对照组的结果。</p></li>
</ul>
<p>作用：让模型学习＂如果处理组样本 <span class="math inline">\(x_i\)</span> 未被处理，结果会怎样＂（反事实推理）。通过 <span class="math inline">\(\hat{\mu}_1-\tau^{\prime}\)</span> 约束处理效应 <span class="math inline">\(\tau^{\prime}\)</span> ，确保效应的一致性。</p>
<p>核心逻辑：反事实约束</p>
<ul>
<li><p>处理效应 <span class="math inline">\(\tau(x)\)</span> 的定义是：<span class="math inline">\(\tau(x)=\mu_1(x)-\mu_0(x)\)</span>（处理后结果－未处理结果）。</p></li>
<li><p>CrossTR／CrossCR 通过反事实假设，强制模型满足：<span class="math inline">\(\mu_1(x)=\mu_0(x)+\tau(x)\)</span>（处理组结果 <span class="math inline">\(=\)</span> 对照组结果 + 处理效应）</p></li>
</ul>
<p><span class="math inline">\(\mu_0(x)=\mu_1(x)-\tau(x)(\)</span> 对照组结果 <span class="math inline">\(=\)</span> 处理组结果 - 处理效应 <span class="math inline">\()\)</span>
－这种约束让模型学习到的处理效应 <span class="math inline">\(\tau(x)\)</span> 更可靠，避免效应预测与直接响应预测矛盾。</p>
<p>总结</p>
<ul>
<li><p>基础损失（ <span class="math inline">\(L_{T R}, L_{C R}\)</span> ）：直接学习＂处理／对照组特征 <span class="math inline">\(\rightarrow\)</span> 结果＂的映射。</p></li>
<li><p>交叉损失（ <span class="math inline">\(L_{\text {CrossTR }}, L_{\text {CrossCR }}\)</span> ）：通过反事实推理，约束＂处理效应 <span class="math inline">\(\tau^{\prime \prime}\)</span> 的合理性，让模型同时满足：</p></li>
<li><p>处理组结果 <span class="math inline">\(=\)</span> 对照组结果 + 处理效应</p></li>
<li><p>对照组结果 <span class="math inline">\(=\)</span> 处理组结果－处理效应</p></li>
<li><p>这种设计是 X－learner 的核心思想，通过反事实约束提升 <span class="math inline">\(5 \vee\)</span> 立预测的准确性，常用于因果推断、uplift modeling 等场景。</p></li>
</ul>
<p>公式中 <span class="math inline">\(\sigma^{-1}\)</span> 表示sigmoid的逆函数，实际上 Cross Treated Response、Cross Control Response 会对 <span class="math inline">\(\tau^{\prime}\)</span> 在 logit 层面做加减，这么做的优点有二：</p>
<p>1．避免数值范围截断：sigmoid 函数的输出范围固定在 <span class="math inline">\((0,1)\)</span> 之间，若直接在概率层面对 <span class="math inline">\(\tau^{\prime}\)</span> 进行加减，可能导致结果超出合理范围（如小于 0 或大于 1），需要额外的截断处理，这会丟失信息或引入偏差。而 logit 层面（ <span class="math inline">\(\sigma^{-1}\)</span> 的输出）是实数域，加减操作不会受边界限制，无需截断。</p>
<p>2．增强小效应信号的捕捉能力：sigmoid 函数的特性是两端斜率极低（接近 0 ），中间区域斜率较高。这意味着在概率接近 0 或 1 时，纵轴（概率）的微小变化对应横轴（logit）的较大变化。通过在 logit 层面操作 <span class="math inline">\(\tau^{\prime}\)</span> ，即使是很小的效应信号，也能通过 logit 空间的显著变化被模型捕捉，从而提升对微弱干预效果的识别能力。</p>
<p><strong>ESN</strong></p>
<p>整体结构如fig1中的图c
DESCN就是将上面ESN（处理Treatment bias的能力）和 X－network（处理Sample imbalance的能力）结合，结构如图（c），损失函数如下：</p>
<p><span class="math display">\[
\begin{aligned}
L_{D E S C N} &amp; =L_{E S N}+\gamma_1 \cdot L_{C r o s s T R}+\gamma_0 \cdot L_{\mathrm{CrossCR}} \\
&amp; =\alpha \cdot L_\pi+\beta_1 \cdot L_{E S T R}+\beta_0 \cdot L_{E S C R} \\
&amp; +\gamma_1 \cdot L_{C r o s s T R}+\gamma_0 \cdot L_{C r o s s C R}
\end{aligned}
\]</span>
效果对比
<img src="refs/descn-res.png" /></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="uplift-model与response-model区别.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="数据收集.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": false,
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
